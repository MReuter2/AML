{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c6818b",
   "metadata": {},
   "source": [
    "<img src=\"https://www.th-koeln.de/img/logo.svg\" style=\"float:right;\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58702112",
   "metadata": {},
   "source": [
    "# 3rd exercise: <font color=\"#C70039\">Do DBScan clustering for anomaly detection</font>\n",
    "* Course: AML\n",
    "* Lecturer: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Author of notebook: Marvin Reuter\n",
    "* Matriculation number: 11139466\n",
    "* Date:   10.11.2024\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/DBSCAN-Illustration.svg/400px-DBSCAN-Illustration.svg.png\" style=\"float: center;\" width=\"450\">\n",
    "\n",
    "---------------------------------\n",
    "**GENERAL NOTE 1**: \n",
    "Please make sure you are reading the entire notebook, since it contains a lot of information on your tasks (e.g. regarding the set of certain paramaters or a specific computational trick), and the written mark downs as well as comments contain a lot of information on how things work together as a whole. \n",
    "\n",
    "**GENERAL NOTE 2**: \n",
    "* Please, when commenting source code, just use English language only. \n",
    "* When describing an observation please use English language, too\n",
    "* This applies to all exercises throughout this course.  \n",
    "\n",
    "---------------------\n",
    "\n",
    "### <font color=\"ce33ff\">DESCRIPTION</font>:\n",
    "This notebook allows you for using the DBScan clustering algorithm for anomaly detection.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### <font color=\"FFC300\">TASKS</font>:\n",
    "The tasks that you need to work on within this notebook are always indicated below as bullet points. \n",
    "If a task is more challenging and consists of several steps, this is indicated as well. \n",
    "Make sure you have worked down the task list and commented your doings. \n",
    "This should be done by using markdown.<br> \n",
    "<font color=red>Make sure you don't forget to specify your name and your matriculation number in the notebook.</font>\n",
    "\n",
    "**YOUR TASKS in this exercise are as follows**:\n",
    "1. import the notebook to Google Colab or use your local machine.\n",
    "2. make sure you specified you name and your matriculation number in the header below my name and date. \n",
    "    * set the date too and remove mine.\n",
    "3. read the entire notebook carefully \n",
    "    * add comments whereever you feel it necessary for better understanding\n",
    "    * run the notebook for the first time. \n",
    "4. take the three data sets from exercize 1 and cluster them\n",
    "5. read the following <a href=\"https://stats.stackexchange.com/questions/88872/a-routine-to-choose-eps-and-minpts-for-dbscan\">article</a> for getting help estimating eps and minPts\n",
    "    * https://stats.stackexchange.com/questions/88872/a-routine-to-choose-eps-and-minpts-for-dbscan\n",
    "6. describe your findings and interpret the results\n",
    "-----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "id": "522a8e3c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:06.938869Z",
     "start_time": "2025-01-02T18:05:05.604475Z"
    }
   },
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "np.random.seed(1)\n",
    "random_data = np.random.randn(50000,2)  * 20 + 20"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "0641d0d0",
   "metadata": {},
   "source": [
    "The output of the below code is 94. This is the total number of noisy points. SKLearn labels the noisy points as (-1). The downside with this method is that the higher the dimension, the less accurate it becomes. You also need to make a few assumptions like estimating the right value for eps which can be challenging."
   ]
  },
  {
   "cell_type": "code",
   "id": "bb98bcc3",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:12.750016Z",
     "start_time": "2025-01-02T18:05:10.404290Z"
    }
   },
   "source": [
    "# hyperparameters\n",
    "minPts = 2\n",
    "eps = 3\n",
    "\n",
    "outlier_detection = DBSCAN(min_samples = minPts, eps = eps)\n",
    "\n",
    "clusters = outlier_detection.fit_predict(random_data)\n",
    "\n",
    "list(clusters).count(-1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TASK",
   "id": "9ba82ea20017df7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:06:21.966747Z",
     "start_time": "2025-01-02T18:06:21.864355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import all datasets\n",
    "import pandas as pd\n",
    "\n",
    "spotify = pd.read_csv(\"./data/kaggle/Most Streamed Spotify Songs 2024.csv\", thousands=',', encoding='ISO-8859-1')\n",
    "height = pd.read_csv(\"./data/kaggle/SOCR-HeightWeight.csv\")\n",
    "birthrate = pd.read_csv(\"./data/kaggle/europe.csv\")"
   ],
   "id": "6987f54893a98eb0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:38:59.647940Z",
     "start_time": "2025-01-02T18:38:59.613276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "streams = spotify['Spotify Streams'].dropna()\n",
    "heights = height['Height(Inches)'].dropna()\n",
    "birthrates = birthrate['birth_rate'].dropna()\n",
    "# Reshape the data to a 2D array\n",
    "streams_reshaped = streams.values.reshape(-1,1)\n",
    "heights_reshaped = heights.values.reshape(-1,1)\n",
    "birthrates_reshaped = birthrates.values.reshape(-1,1)"
   ],
   "id": "fde2a73046538533",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:49:43.288781Z",
     "start_time": "2025-01-02T18:49:30.127684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "std_dev = np.std(heights_reshaped)\n",
    "print(std_dev)\n",
    "print(heights_reshaped.size)\n",
    "\n",
    "# hyperparameters\n",
    "minPts = 2000\n",
    "eps = std_dev  # As noted in the original comment, this value might need tuning\n",
    "\n",
    "outlier_detection = DBSCAN(min_samples=minPts, eps=eps)\n",
    "clusters = outlier_detection.fit_predict(heights_reshaped)\n",
    "\n",
    "num_outliers = list(clusters).count(-1)\n",
    "print(f\"Number of outliers: {num_outliers}\")"
   ],
   "id": "daf2d546f1b68f38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9016407372498432\n",
      "25000\n",
      "Number of outliers: 14\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:49:20.051941Z",
     "start_time": "2025-01-02T18:49:19.732487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "std_dev = np.std(streams_reshaped)\n",
    "print(std_dev)\n",
    "print(streams_reshaped.size)\n",
    "\n",
    "# hyperparameters\n",
    "minPts = 200\n",
    "eps = std_dev  # As noted in the original comment, this value might need tuning\n",
    "\n",
    "outlier_detection = DBSCAN(min_samples=minPts, eps=eps)\n",
    "clusters = outlier_detection.fit_predict(streams_reshaped)\n",
    "\n",
    "num_outliers = list(clusters).count(-1)\n",
    "print(f\"Number of outliers: {num_outliers}\")"
   ],
   "id": "b219c5135d20a75e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538383901.5027055\n",
      "4487\n",
      "Number of outliers: 22\n"
     ]
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:49:12.387200Z",
     "start_time": "2025-01-02T18:49:12.366433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "std_dev = np.std(birthrates_reshaped)\n",
    "print(std_dev)\n",
    "print(birthrates_reshaped.size)\n",
    "\n",
    "# hyperparameters\n",
    "minPts = 10\n",
    "eps = std_dev  # As noted in the original comment, this value might need tuning\n",
    "\n",
    "outlier_detection = DBSCAN(min_samples=minPts, eps=eps)\n",
    "clusters = outlier_detection.fit_predict(birthrates_reshaped)\n",
    "\n",
    "num_outliers = list(clusters).count(-1)\n",
    "print(f\"Number of outliers: {num_outliers}\")"
   ],
   "id": "ea5d01bb31203d5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0943845161257548\n",
      "36\n",
      "Number of outliers: 2\n"
     ]
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Findings & results\n",
    "\n",
    "After testing several values, the ones I used gave the best results (compared to what I expected). It was difficult to find a suitable eps. Using three times the standard deviation as epsilon resulted in overly large clusters, effectively grouping almost all data points into a single cluster and failing to identify outliers. So I decided to use just the standard deviation and it works somehow. More systematic approaches to determining epsilon include the k-distance graph method, which can help identify a characteristic 'knee' in the distance plot, suggesting an appropriate epsilon value.\n",
    "\n",
    "In conclusion, it's hard to find good values by just guessing and trying different values. It's also hard to say whether a result is good or not so good. "
   ],
   "id": "ed72f251db7e8198"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
